{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f98a56c-863b-48e8-88f8-9802cfd285ee",
   "metadata": {},
   "source": [
    "## word2vec을 이용한 모델\n",
    "- word2vec은 단어로 표현된 리스트를 입력 값으로 넣어야함\n",
    "- 전처리 된 테스트를 불러온 후 각 단어들의 리스트로 나누어야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29e032c-c664-4749-a645-70b1818b1c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47cd3229-591f-4c9a-a217-205f0a6493b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stuff going moment mj started listening music ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classic war worlds timothy hines entertaining ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>film starts manager nicholas bell giving welco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>must assumed praised film greatest filmed oper...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>superbly trashy wondrously unpretentious explo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  stuff going moment mj started listening music ...          1\n",
       "1  classic war worlds timothy hines entertaining ...          1\n",
       "2  film starts manager nicholas bell giving welco...          0\n",
       "3  must assumed praised film greatest filmed oper...          0\n",
       "4  superbly trashy wondrously unpretentious explo...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "TRAIN_CLEAN_DATA = 'train_clean1.csv'\n",
    "\n",
    "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc7e002b-cc52-4e3f-852b-7da290e2d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ff7607-fb85-46e7-a437-bbfe36960727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95e2687f-6a9b-4ae7-baf1-c2b06cde4883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter\n"
     ]
    }
   ],
   "source": [
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0b3f470-e44e-4b73-9009-93f621fa179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 문장문장을 단어들로 변환\n",
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e93432-3270-490e-88f5-d6000a1acc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb6c6da-bb67-49fe-bebc-d7a8da1fa0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent', 'moonwalker', 'part', 'biography', 'part', 'feature', 'film', 'remember', 'going', 'see', 'cinema', 'originally', 'released', 'subtle', 'messages', 'mj', 'feeling', 'towards', 'press', 'also', 'obvious', 'message', 'drugs', 'bad', 'kay', 'visually', 'impressive', 'course', 'michael', 'jackson', 'unless', 'remotely', 'like', 'mj', 'anyway', 'going', 'hate', 'find', 'boring', 'may', 'call', 'mj', 'egotist', 'consenting', 'making', 'movie', 'mj', 'fans', 'would', 'say', 'made', 'fans', 'true', 'really', 'nice', 'actual', 'feature', 'film', 'bit', 'finally', 'starts', 'minutes', 'excluding', 'smooth', 'criminal', 'sequence', 'joe', 'pesci', 'convincing', 'psychopathic', 'powerful', 'drug', 'lord', 'wants', 'mj', 'dead', 'bad', 'beyond', 'mj', 'overheard', 'plans', 'nah', 'joe', 'pesci', 'character', 'ranted', 'wanted', 'people', 'know', 'supplying', 'drugs', 'etc', 'dunno', 'maybe', 'hates', 'mj', 'music', 'lots', 'cool', 'things', 'like', 'mj', 'turning', 'car', 'robot', 'whole', 'speed', 'demon', 'sequence', 'also', 'director', 'must', 'patience', 'saint', 'came', 'filming', 'kiddy', 'bad', 'sequence', 'usually', 'directors', 'hate', 'working', 'one', 'kid', 'let', 'alone', 'whole', 'bunch', 'performing', 'complex', 'dance', 'scene', 'bottom', 'line', 'movie', 'people', 'like', 'mj', 'one', 'level', 'another', 'think', 'people', 'stay', 'away', 'try', 'give', 'wholesome', 'message', 'ironically', 'mj', 'bestest', 'buddy', 'movie', 'girl', 'michael', 'jackson', 'truly', 'one', 'talented', 'people', 'ever', 'grace', 'planet', 'guilty', 'well', 'attention', 'gave', 'subject', 'hmmm', 'well', 'know', 'people', 'different', 'behind', 'closed', 'doors', 'know', 'fact', 'either', 'extremely', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liars', 'hope', 'latter']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b475eb4-3d92-41c0-8779-cdd0890a8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c anaconda gensim=3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19250646-2df9-4c56-a27f-bb5bb318eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features =300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70852f65-6e4e-4787-8f25-e24a0b12b694",
   "metadata": {},
   "source": [
    "- num_features : 각 단어에 대해 임베딩된 벡터의 차원 지정(feature 수)\n",
    "- min_word_count : 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않기 위해 설정  \n",
    "- num_workers : 모델 학습 시 학습을 위한 쓰레드 수 지정(기본값 3)  \n",
    "- context : word2vec 을 수행하기 위한 컨텍스트 윈도우 사이즈 지정  \n",
    "a. Maximum distance between the current and predicted word within a sentence.  \n",
    "b. 기준 단어의 앞뒤에 존재하는 단어들로 기준 단어를 예측하게 되는데(sg=0, CBOW-Continuous Bag of Words)  \n",
    "c. 이 때 기준 단어에서 앞뒤 얼마나 떨어져 있는 단어까지 고려하는가를 결정\n",
    "- downsampling : word2vec 학습을 수행할 때 빠른 학습을 위해 정답 단어 레이블에 대한 다운샘플링 비율을 지정  \n",
    "a. 보통 0.001이 좋은 성능을 낸다고 알려짐  \n",
    "b. 0.001 값을 threshold 값으로 보고, 이 값보다 빈도수가 높은 단어들은 무작위로(랜덤) 다운샘플링 됨  \n",
    "c. 빈도수가 높은 단어는 다운샘플링하여 가끔 학습(랜덤하게 무시)하고 빈도수가 낮은 단어는 출현 족족 학습하는 효과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81eb42bc-0629-4c28-a445-1d9e946d9478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0284873f-27db-403a-bc69-69c23511bd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-08 14:18:36,090 : INFO : collecting all words and their counts\n",
      "2021-10-08 14:18:36,090 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-10-08 14:18:36,286 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2021-10-08 14:18:36,487 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2021-10-08 14:18:36,574 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2021-10-08 14:18:36,575 : INFO : Loading a fresh vocabulary\n",
      "2021-10-08 14:18:36,612 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2021-10-08 14:18:36,612 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
      "2021-10-08 14:18:36,634 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2021-10-08 14:18:36,636 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2021-10-08 14:18:36,637 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
      "2021-10-08 14:18:36,653 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2021-10-08 14:18:36,653 : INFO : resetting layer weights\n",
      "2021-10-08 14:18:37,882 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2021-10-08 14:18:38,892 : INFO : EPOCH 1 - PROGRESS: at 48.74% examples, 1215852 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:18:39,898 : INFO : EPOCH 1 - PROGRESS: at 94.82% examples, 1177002 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:18:40,006 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:18:40,009 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:18:40,015 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:18:40,035 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:18:40,036 : INFO : EPOCH - 1 : training on 2988089 raw words (2494129 effective words) took 2.2s, 1160051 effective words/s\n",
      "2021-10-08 14:18:41,047 : INFO : EPOCH 2 - PROGRESS: at 44.81% examples, 1115660 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:18:42,049 : INFO : EPOCH 2 - PROGRESS: at 93.84% examples, 1165584 words/s, in_qsize 8, out_qsize 0\n",
      "2021-10-08 14:18:42,152 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:18:42,159 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:18:42,160 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:18:42,162 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:18:42,162 : INFO : EPOCH - 2 : training on 2988089 raw words (2494722 effective words) took 2.1s, 1175083 effective words/s\n",
      "2021-10-08 14:18:43,167 : INFO : EPOCH 3 - PROGRESS: at 45.44% examples, 1138391 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:18:44,169 : INFO : EPOCH 3 - PROGRESS: at 92.05% examples, 1147886 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:18:44,324 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:18:44,331 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:18:44,333 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:18:44,339 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:18:44,339 : INFO : EPOCH - 3 : training on 2988089 raw words (2494095 effective words) took 2.2s, 1146640 effective words/s\n",
      "2021-10-08 14:18:45,344 : INFO : EPOCH 4 - PROGRESS: at 45.16% examples, 1130850 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:18:46,351 : INFO : EPOCH 4 - PROGRESS: at 88.98% examples, 1109193 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:18:46,656 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:18:46,661 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:18:46,666 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:18:46,675 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:18:46,675 : INFO : EPOCH - 4 : training on 2988089 raw words (2494549 effective words) took 2.3s, 1069154 effective words/s\n",
      "2021-10-08 14:18:47,688 : INFO : EPOCH 5 - PROGRESS: at 33.15% examples, 827202 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:18:48,709 : INFO : EPOCH 5 - PROGRESS: at 68.38% examples, 844511 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:18:49,577 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:18:49,581 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:18:49,588 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:18:49,599 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:18:49,600 : INFO : EPOCH - 5 : training on 2988089 raw words (2494273 effective words) took 2.9s, 853916 effective words/s\n",
      "2021-10-08 14:18:49,600 : INFO : training on a 14940445 raw words (12471768 effective words) took 11.7s, 1064414 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                          size=num_features, min_count=min_word_count,\n",
    "                          window=context, sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef903e72-9254-4052-9789-f2c402f1f307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-08 14:18:49,622 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2021-10-08 14:18:49,622 : INFO : not storing attribute vectors_norm\n",
      "2021-10-08 14:18:49,624 : INFO : not storing attribute cum_table\n",
      "2021-10-08 14:18:49,822 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a54eb-249e-4a80-baf6-c3c9d13ad4f7",
   "metadata": {},
   "source": [
    "- 위에서 만든word2vec 모델을 활용하여 선형 회귀 모델을 학습시켜봄\n",
    "- 각 리뷰를 같은 형태의 입력값으로 만들어야 함\n",
    "- 리뷰마다 단어의 수가 모두 다르므로 입력값을 하나의 형태로 만듬\n",
    "- 가장 단순한 방법으로, 문장에 있는 모든 단어의 벳터값에 대해 평균을 내서 리뷰 하나당 하나의 벡터로 만드는 방법을 사용하겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5497efbd-c87e-4b34-a5c3-3074922579b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(words, model, num_features):\n",
    "    feature_vector = np.zeros((num_features), dtype=np.float32)\n",
    "    \n",
    "    num_words = 0\n",
    "    index2word_set = set(model.wv.index2word)#wv.index2word단어사전\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words += 1\n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "    #wv.index2word사전에서 하나씩 꺼내 사전안에 존재하면 하나씩 더해준다        \n",
    "    feature_vector = np.divide(feature_vector, num_words)\n",
    "    #벡터값을 다 더해서 평균내준다\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f185d716-199d-41b1-82d9-7aff86180e8b",
   "metadata": {},
   "source": [
    "- words : 단어의 모음인 하나의 리뷰가 들어감\n",
    "- model : word2vec 모델\n",
    "- num_features : word2vec 으로 임베딩할 때 정했던 벡터의 차원 수  \n",
    "- 결극 하나의 문장에 등장하는 사전에 등록된 단어들의 벡터값의 평균을 구함  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12a7a84b-7de1-4395-b3aa-a033cbf76333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "    \n",
    "    for s in reviews:\n",
    "        dataset.append(get_features(s, model, num_features))\n",
    "        \n",
    "    reviewFeatureVecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c95922-8cd7-418a-9c26-8c6540ec3a95",
   "metadata": {},
   "source": [
    "- review : 전체 리뷰 데이터  \n",
    "- model : 학습시킨 모델  \n",
    "- num_features : word2vec 임베딩 시 정했던 벡터의 차원 수  \n",
    "- np.stack(dataset, axis=0) 은 row 로 데이터를 쌓으면서 numpy 배열을 만든다는 의미  \n",
    "- 이렇게 하여 row가 전체 샘플 수 만큼, column 은 feature의 차원 수가 됨  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97a3b2bb-cb6d-49fd-aca1-27010f672072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seulki\\AppData\\Local\\Temp/ipykernel_6608/581192012.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  feature_vector = np.add(feature_vector, model[w])\n"
     ]
    }
   ],
   "source": [
    "train_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8559a71e-c27c-4bf9-8107-99d361e2b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_data_vecs\n",
    "y = np.array(sentiments)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "                train_test_split(X,y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be422c09-48a9-427d-b9e2-de8d1218f626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seulki\\anaconda3\\envs\\nltk\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lgs = LogisticRegression(class_weight = 'balanced')\n",
    "lgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93cdce64-089b-47fe-9b9b-67a3cea62a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.865600\n"
     ]
    }
   ],
   "source": [
    "predicted = lgs.predict(X_test)\n",
    "print('Accuracy: %f' % lgs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "160216e4-d7e6-430b-a9ce-d7ec0ae62e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
    "test_review = list(test_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f59d5aaa-db08-45cf-aa44-c1d1fc352db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = list()\n",
    "for review in test_review:\n",
    "    test_sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8407d66a-0969-4aa1-8bb0-bd057a95c8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seulki\\AppData\\Local\\Temp/ipykernel_6608/581192012.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  feature_vector = np.add(feature_vector, model[w])\n"
     ]
    }
   ],
   "source": [
    "test_data_vecs = get_dataset(test_sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "958a6ba4-1c1e-4f06-97d9-4dfe3a1e8f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = lgs.predict(test_data_vecs)\n",
    "\n",
    "ids = list(test_data['id'])\n",
    "answer_dataset = pd.DataFrame({'id' : ids, 'sentiment':test_predicted})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10b9a35c-cd76-458d-85c7-0171b6cc5028",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "    \n",
    "answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_w2v_answer.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d30a1c-d066-42d9-810a-c20af35a9f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
